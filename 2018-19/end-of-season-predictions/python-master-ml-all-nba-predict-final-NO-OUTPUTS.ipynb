{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the datasets\n",
    "\n",
    "dfHistorical = pd.read_csv('historical-all-nba.csv')\n",
    "dfCurrent = pd.read_csv('current-all-nba.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHistorical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(dfHistorical, test_size = 0.25, random_state = 36)\n",
    "\n",
    "xtrain = train[['Team Wins', 'Overall Seed', 'PTS', 'TRB', 'AST', 'VORP', 'WS', 'All-Star']]\n",
    "ytrain = train[['All-NBA']]\n",
    "\n",
    "xtest = test[['Team Wins', 'Overall Seed', 'PTS', 'TRB', 'AST', 'VORP', 'WS', 'All-Star']]\n",
    "ytest = test[['All-NBA']]\n",
    "\n",
    "print(\"Training set size: %.0f\" % len(xtrain))\n",
    "print(\"Testing set size: %.0f\" % len(xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that gives accuracy scores for each model\n",
    "\n",
    "def scores(model):\n",
    "    \n",
    "    model.fit(xtrain, ytrain.values.ravel())\n",
    "    y_pred = model.predict(xtest)\n",
    "    \n",
    "    print(\"Accuracy score: %.3f\" % metrics.accuracy_score(ytest, y_pred))\n",
    "    print(\"Recall: %.3f\" % metrics.recall_score(ytest, y_pred))\n",
    "    print(\"Precision: %.3f\" % metrics.precision_score(ytest, y_pred))\n",
    "    print(\"F1: %.3f\" % metrics.f1_score(ytest, y_pred))\n",
    "    \n",
    "    proba = model.predict_proba(xtest)\n",
    "    print(\"Log loss: %.3f\" % metrics.log_loss(ytest, proba))\n",
    "\n",
    "    pos_prob = proba[:, 1]\n",
    "    print(\"Area under ROC curve: %.3f\" % metrics.roc_auc_score(ytest, pos_prob))\n",
    "    \n",
    "    cv = cross_val_score(model, xtest, ytest.values.ravel(), cv = 3, scoring = 'accuracy')\n",
    "    print(\"Accuracy (cross validation score): %0.3f (+/- %0.3f)\" % (cv.mean(), cv.std() * 2))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel = 'rbf', gamma = 1e-3, C = 100, probability = True)\n",
    "\n",
    "y_svc = scores(svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state = 999, n_estimators = 100, criterion = 'gini')\n",
    "\n",
    "y_rf = scores(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 12, weights = 'uniform')\n",
    "\n",
    "y_knn = scores(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = MLPClassifier(solver = 'lbfgs', hidden_layer_sizes = 100, random_state = 999, activation = 'relu')\n",
    "\n",
    "y_dnn = scores(dnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCurrentNames = dfCurrent.iloc[:, 0]\n",
    "dfCurrentPredict = dfCurrent[['Team Wins', 'Overall Seed', 'PTS', 'TRB', 'AST', 'VORP', 'WS', 'All-Star']]\n",
    "\n",
    "dfCurrent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred(model):\n",
    "\n",
    "    proba = model.predict_proba(dfCurrentPredict)\n",
    "    pos_prob = proba[:, 1]\n",
    "    \n",
    "    combined_list = [[i, j] for i, j in zip(dfCurrentNames, pos_prob)]\n",
    "    combined_list = sorted(combined_list, key = itemgetter(1), reverse = True)\n",
    "    \n",
    "    for i in combined_list:\n",
    "        print(i)\n",
    "        \n",
    "    return pos_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_prob = make_pred(svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_prob = make_pred(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_prob = make_pred(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_prob = make_pred(dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_prob = []\n",
    "\n",
    "for i, j, k, l in zip(svc_prob, rf_prob, knn_prob, dnn_prob):\n",
    "    avg_prob.append((i + j + k + l) / 4)\n",
    "    \n",
    "avg_list = [[i, j] for i, j in zip(dfCurrentNames, avg_prob)]\n",
    "avg_list = sorted(avg_list, key = itemgetter(1), reverse = True)\n",
    "\n",
    "for i in avg_list:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
